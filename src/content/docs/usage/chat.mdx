---
title: LLM Chat & MCP
description: Expose your email archive to AI assistants via MCP, or query it with the built-in chat REPL.
---

Both the `mcp` server and the `chat` command operate on your **local, read-only archive**, not your live Gmail account. The AI cannot send emails, modify labels, delete messages, or access your Google credentials. You control when data enters the archive (via `sync-full` / `sync-incremental`) and when anything is deleted (via the explicit [deletion workflow](/usage/deletion/)). Compared to giving an AI assistant direct OAuth access to your mailbox, this is a fundamentally smaller attack surface.

## MCP Server

The `mcp` command starts a [Model Context Protocol](https://modelcontextprotocol.io/) (MCP) server that exposes your email archive as a set of tools. This lets AI assistants like Claude Desktop search, read, and analyze your messages directly.

### Claude Desktop Configuration

Add the following to your Claude Desktop config file:

- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
- **Windows**: `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "msgvault": {
      "command": "msgvault",
      "args": ["mcp"]
    }
  }
}
```

If `msgvault` is not on your PATH, use the full path to the binary. Restart Claude Desktop after saving the config.

### Available Tools

The MCP server exposes the following tools to connected AI clients:

| Tool | Description | Parameters |
|---|---|---|
| `search_messages` | Full-text search with Gmail-like query syntax | `query` (string), `max_results` (int, optional) |
| `get_message` | Retrieve a single message by ID | `message_id` (int) |
| `list_messages` | List messages with filters | `from` (string), `to` (string), `label` (string), `before` (date), `after` (date), `limit` (int) |
| `get_stats` | Archive statistics | — |
| `aggregate` | Run aggregate queries (top senders, domains, labels, time series) | `group_by` (string), `limit` (int), `account` (string, optional) |

### Example Usage with Claude

Once configured, you can ask Claude questions like:

- *"Search my email for messages from alice@example.com about the project proposal"*
- *"How many emails did I receive last month?"*
- *"Show me the top 10 senders in my archive"*
- *"Find all messages with attachments larger than 5MB"*

Claude will automatically call the appropriate msgvault tools to retrieve and analyze your messages.

### CLI Flags

```bash
# Start the MCP server (stdio transport)
msgvault mcp

# Force SQL retrieval for all searches
msgvault mcp --force-sql
```

| Flag | Default | Description |
|---|---|---|
| `--force-sql` | `false` | Always use SQL retrieval instead of FTS5 |

## LLM Chat (Experimental)

The `chat` command launches an interactive REPL that lets you ask natural-language questions about your email archive. It uses retrieval-augmented generation (RAG) to search your messages and synthesize answers. You can use a local model via Ollama or a cloud API like Anthropic or OpenAI.

### How It Works

```
Your question → Query planning → FTS5 / SQL retrieval → Context assembly → LLM answer
```

1. **Query planning**: the LLM analyzes your question and generates one or more search queries
2. **Retrieval**: msgvault runs those queries against the SQLite full-text index (or SQL for aggregate questions)
3. **Answer generation**: retrieved messages are passed as context to the LLM, which produces a grounded answer with citations

### Prerequisites

You need at least one LLM backend configured:

#### Local models (Ollama)

Install [Ollama](https://ollama.com) and pull a model:

```bash
ollama pull llama3.1
ollama list
```

#### Cloud APIs

Set your API key as an environment variable:

```bash
# Anthropic
export ANTHROPIC_API_KEY=sk-ant-...

# OpenAI
export OPENAI_API_KEY=sk-...
```

### Launch

```bash
# Local model via Ollama (default)
msgvault chat

# Use a specific Ollama model
msgvault chat --model llama3.1

# Use Claude via Anthropic API
msgvault chat --provider anthropic --model claude-sonnet-4-20250514

# Use GPT via OpenAI API
msgvault chat --provider openai --model gpt-4o

# Connect to a remote Ollama instance
msgvault chat --server http://remote-host:11434

# Limit retrieval results
msgvault chat --max-results 20

# Force SQL mode for aggregate queries
msgvault chat --force-sql
```

### CLI Flags

| Flag | Default | Description |
|---|---|---|
| `--provider` | `ollama` | LLM provider: `ollama`, `anthropic`, or `openai` |
| `--server` | `http://localhost:11434` | Ollama API endpoint (Ollama provider only) |
| `--model` | `llama3.1` | Model name (provider-specific) |
| `--max-results` | `10` | Maximum messages to retrieve per query |
| `--force-sql` | `false` | Always use SQL retrieval instead of FTS5 |

### Configuration

You can set defaults in your `config.toml` instead of passing flags each time:

```toml
[chat]
provider = "ollama"           # or "anthropic", "openai"
server = "http://localhost:11434"
model = "llama3.1"
max_results = 10
```

See [Configuration](/configuration/) for the full config reference.

### Example Session

```
$ msgvault chat
msgvault chat> Who emailed me the most in 2024?

Searching archive...
Retrieved 10 messages across 8 senders.

Based on your archive, the top sender in 2024 was newsletter@example.com
with 847 messages, followed by alice@company.com with 312 messages.

msgvault chat> What did Alice say about the Q3 budget?

Searching archive...
Retrieved 5 messages matching "Q3 budget" from alice@company.com.

Alice sent three emails about the Q3 budget between July and September:
1. **Jul 12**: Initial proposal requesting $50K for infrastructure [msg:4521]
2. **Aug 3**: Revised numbers after the team meeting [msg:4892]
3. **Sep 1**: Final approval confirmation [msg:5201]

msgvault chat> /quit
```

Type `/quit` or press `Ctrl+D` to exit the REPL.
