---
title: Parquet Analytics
description: Denormalized Parquet cache for lightning-fast TUI analytics powered by DuckDB.
---

## Why Parquet + DuckDB

The TUI needs to aggregate across your entire archive (top senders, domains, labels, time series) and return results instantly as you drill down. SQLite JOINs across normalized tables cannot do this at interactive speeds on large archives.

msgvault solves this with Parquet metadata exports queried by an embedded DuckDB engine. The result is aggregate queries **hundreds of times faster**: what takes seconds in SQLite returns in milliseconds from Parquet. The analytics layer has a small footprint compared to the full SQLite database with message bodies, so it loads instantly and stays out of your way.

## Building the Cache

```bash
# Manual build
msgvault build-cache

# Full rebuild (discard existing)
msgvault build-cache --full-rebuild
```

The TUI automatically builds or updates the cache on launch when new messages are detected.

## Directory Structure

```
~/.msgvault/
├── msgvault.db              # SQLite: system of record
└── analytics/               # Parquet: aggregate analytics
    ├── messages/year=*/     # Partitioned by year
    └── _last_sync.json      # Incremental sync state
```

## Parquet Schema

The Parquet files contain exported message metadata:

- `from_email`, `from_domain`: sender info
- `to_emails[]`, `labels[]`: arrays for recipients and labels
- `subject`, `snippet`: message preview
- `sent_at`, `size_estimate`: metadata
- `has_attachments`, `attachment_count`: attachment info

Partitioned by `year` for efficient time-range queries.

## Size

Approximately 3MB for 268,000 messages, compared to ~1GB for SQLite with full message bodies.

## DuckDB Engine

The TUI uses an embedded DuckDB engine (`internal/query/engine.go`) to query Parquet files directly. This avoids loading data into memory and supports efficient aggregate operations over partitioned data.
